{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we'll explore the Transformer architecture, a neural network that takes advantage of parallel processing and allows you to substantially speed up the training process. \n",
    "\n",
    "**After this Project you'll be able to**:\n",
    "* Understand each step of Transformer Network\n",
    "* understand Attention\n",
    "* how to change paper to code \n",
    "\n",
    "### What is Transformer Model? \n",
    "The Transformer model, introduced in Attention Is All You Need (2017), is a deep learning architecture that replaces recurrence with self-attention to process sequences in parallel.\n",
    "* the paper link [attention is all you need](https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "<img src=\"images/Transformer.png\" alt=\"Encoder\" width=\"600\"/>\n",
    "<caption><center><font color='Green'><b>Figure 1:  The Transformer- model architecture.</font></center></caption>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, MultiHeadAttention, Dense, Input, Dropout, LayerNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Token Embedding\n",
    "\n",
    "This layer converts token indices into dense vectors, similar to how PyTorch/TensorFlow embedding layers work.\n",
    "\n",
    "<a name='6'></a>\n",
    "### 3.1 - He Initialization\n",
    "\n",
    "Finally, try \"He Initialization\"; this is named for the first author of He et al., 2015. \n",
    "\n",
    "\n",
    "Implement the following function to initialize our parameters with He initialization.  $\\sqrt{\\frac{2}{\\text{dimension of the previous layer}}}$, which is what He initialization recommends for layers with a ReLU activation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding:\n",
    "    def __init__(self, VOCAB_SIZE, d_model):\n",
    "\n",
    "        # VOCAB_SIZE: total number of unique tokens which means total vocabulary \n",
    "        # d_model: dimension  of each token's embedding vector \n",
    "        \n",
    "        # initialize imbedding matrix with xavier method \n",
    "        self.VOCAB_SIEZ = VOCAB_SIZE\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.embedding_matrix = np.random.randn(VOCAB_SIZE, d_model) * np.sqrt(2/d_model)\n",
    "\n",
    "    def toDenseVector(self, TOKEN_INDICES):\n",
    "        # this change the token to its represented embedding matrix\n",
    "        return self.embedding_matrix[TOKEN_INDICES]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEXT CASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Embeddings Shape: (2, 5, 128)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' \\n2 => Batch size (in this case 2 sentence at once)\\n5 => Sequence Length ( 5 word on each sentence )\\n128 => embedding dense \\n\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE = 10000\n",
    "d_model = 128\n",
    "token_embedding = TokenEmbedding(VOCAB_SIZE, d_model)\n",
    "\n",
    "token_indices = np.array([\n",
    "    [1, 5, 7, 2, 8],  # First sequence\n",
    "    [4, 3, 9, 6, 0]   # Second sequence\n",
    "])\n",
    "\n",
    "embedded_tokens = token_embedding.toDenseVector(token_indices)\n",
    "print(\"Token Embeddings Shape:\", embedded_tokens.shape)  # Expected: (2, 5, 128) \n",
    "\n",
    "\"\"\" \n",
    "2 => Batch size (in this case 2 sentence at once)\n",
    "5 => Sequence Length ( 5 word on each sentence )\n",
    "128 => embedding dense \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Positional Encoding\n",
    "\n",
    "In sequence to sequence tasks, the relative order of our data is extremely important to its meaning. When we were training sequential neural networks such as RNNs, we fed our inputs into the network in order. Information about the order of our data was automatically fed into our model.  However, when we train a Transformer network using multi-head attention, we feed our data into the model all at once. While this dramatically reduces training time, there is no information about the order of our data. This is where positional encoding is useful - we can specifically encode the positions of our inputs and pass them into the network using these sine and cosine formulas:\n",
    "$$\n",
    "PE_{(pos, 2i)}= sin\\left(\\frac{pos}{{10000}^{\\frac{2i}{d}}}\\right)\n",
    "\\tag{1}$$\n",
    "\n",
    "* $d$ is the dimension of the word embedding and positional encoding\n",
    "* $pos$ is the position of the word.\n",
    "* $k$ refers to each of the different dimensions in the positional encodings, with $i$ equal to $k$ $//$ $2$.\n",
    "  \n",
    "This ensures each position has a unique encoding and helps the model differentiate between words based on their order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class PositionalEncoding:\n",
    "    def __init__(self, max_seq_length, d_model):\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.d_model = d_model\n",
    "        self.positional_encoding = self.compute_positional_encoding(max_seq_length, d_model)\n",
    "\n",
    "    def compute_positional_encoding(self, max_seq_length, d_model):\n",
    "        position = np.arange(max_seq_length)[:, np.newaxis]\n",
    "        div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
    "        pos_enc = np.zeros((max_seq_length, d_model))\n",
    "        pos_enc[:, 0::2] = np.sin(position * div_term)\n",
    "        pos_enc[:, 1::2] = np.cos(position * div_term)\n",
    "        return pos_enc\n",
    "\n",
    "    def forward(self, token_embeddings):\n",
    "        batch_size, seq_length, d_model = token_embeddings.shape\n",
    "        assert d_model == self.d_model, \"Embedding dimension must match d_model\"\n",
    "        return token_embeddings + self.positional_encoding[:seq_length, :]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST CASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional Encoded Shape: (2, 5, 128)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "max_seq_length = 100  # Maximum sequence length\n",
    "d_model = 128        # Embedding dimension\n",
    "\n",
    "pos_enc = PositionalEncoding(max_seq_length, d_model)\n",
    "\n",
    "# Generate dummy token embeddings (batch_size=2, seq_length=5, d_model=128)\n",
    "token_embeddings = np.random.randn(2, 5, d_model)\n",
    "\n",
    "# Add positional encoding\n",
    "encoded_tokens = pos_enc.forward(token_embeddings)\n",
    "\n",
    "print(\"Positional Encoded Shape:\", encoded_tokens.shape)  # Expected: (2, 5, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Scaled Dot Product Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. FeedForward Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Layer Normalization & Skip Connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Transformer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
